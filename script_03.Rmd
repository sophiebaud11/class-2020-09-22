---
title: "Week 3"
author: "David Kane"
output: html_document
---

Go to https://registrar.fas.harvard.edu/faculty-staff/courses/enrollment and scroll down to "Access the Current Course Enrollment Numbers." Click on that to download the Excel file. Create a folder in your project called `new_data`. Move the Excel file into that folder. Note that, even if you did this last week, you are doing it again because Harvard has updated the file. The file might be dated either September 21 or 22. We won't know till class!

Note that I have already created a directory called `old_data` and included the file from September 1 in it, along with other data which I have collected. Because I am your buddy, I even give you the code for reading it in! (Although I did leave one mistake for you to find . . .)

Load **tidyverse**, **readxl* and **janitor** into your setup chunk.

```{r setup, include=FALSE}
library(tidyverse)
library(readxl)
library(janitor)
knitr::opts_chunk$set(echo = TRUE)
```


### Scene 0

**Prompt:**  First, figure out what is wrong with the `sep_old` object. Edit the code below to fix it. Read the comments for hints.

```{r sc0}
# Most groups got something like this going last week. Note the use of skip = 3
# to get rid of the garbage rows at the top of the file. Note the is.na()
# filter, which gets rid of the rows at the bottom, especially the dangerous
# summary row. Raw excel sheets are dangerous! Note that it was easy to naively
# assume that there was only one row per class. Untrue!

sep_old <- 
  read_excel("old_data/class_enrollment_summary_by_term_9-1-2020.xlsx", 
             skip = 3) %>% 
  clean_names() %>% 
  filter(! is.na(course_title)) %>% 
  select(-grad, -non_degree, -x_reg, -vus, -employee, 
         -withdraw, -total, -instructor_full_name, -course_section_code) %>%
  rename(id = course_id,
         title = course_title,
         name = course_name,
         department = course_department) %>% 
  filter(u_grad > 10) %>%
  group_by(title) %>%
  summarize(id, name, department, u_grad = sum(u_grad), .groups = "drop") %>%
  distinct()
sep_old
# But this is not correct! Look for Gov 50: Data. What do you see? What can you
# do to clean it up?
```


### Scene 1

**Prompt:** Read in and clean the new data, creating an object called `sep_new`. 

``` {r}
sep_new <- 
  read_excel("new_data/class_enrollment_summary_by_term_9-22-2020.xlsx",
             skip = 3) %>%
    clean_names() %>% 
  filter(! is.na(course_title)) %>% 
  select(-grad, -non_degree, -x_reg, -vus, -employee, 
         -withdraw, -total, -instructor_full_name, -course_section_code) %>%
  rename(id = course_id,
         new_title = course_title,
         new_name = course_name,
         new_department = course_department,
         new_u_grad = u_grad) %>% 
  filter(new_u_grad > 10) %>%
  group_by(new_title) %>%
  summarize(id, new_name, new_department, new_u_grad = sum(new_u_grad), .groups = "drop") %>%
  distinct()
sep_new
```


### Scene 2

**Prompt:** Dean Amanda Claybaugh is concerned about the drop in undergraduate enrollment in some courses between September 1 and today. She wants you to analyze this issue. Before you dive into the details, provide some bullet points as to how Wisdom and Temperance apply to this situation. Every student should have several bullet points. Someone may be asked to share their screen and discuss. Refer to the Chapter 3 discussions about these Cardinal Virtues. Open it up in your browser.


Before we start on the model, Wisdom suggests we should:

Clean and organize the data and make sure it contains exactly the variables we want to work with, so that we don't run into issues with untidy data later on.
We also want to figure out what exactly we hope to do with & learn fromthe data.

After we have a model, Temperance suggests we should:

Recognize and explain the limitations of our model, and be clear & honest about
what your model is doing & saying.


### Scene 3

**Prompt:** Which classes had the biggest increases and decreases in undergraduate enrollments between September 1 and today? Make a graphic that shows the 5 biggest increases and decreases. Make it look nice.

```{r}
merged_data <- 
  merge(sep_old, sep_new, by = "id") %>%
  mutate(en_change_pct = ((new_u_grad - u_grad) / u_grad) * 100) %>% 
  arrange(desc(en_change_pct)) %>%
  slice(1:5, 392:396) %>%
  mutate(is_incr = if_else(condition = en_change_pct > 0, true = "Y", 
                           false = "N")) %>%
  ggplot(aes(x = reorder(title, en_change_pct), 
             y = en_change_pct, fill = is_incr)) + geom_col() + 
  theme(axis.text.x = element_text(angle = 90)) + 
  labs(title = "Biggest Enrollment Increases and Decreases in September 2020") +
  xlab("Class") + ylab("Percent Change in Enrollment") + 
  scale_color_manual(breaks = c("Y", "N"), values = c("green3", "red3"), 
                     aesthetics = "fill", 
                     labels = c("Enrollment Increase",
                                "Enrollment Decrease"), name = "")
merged_data
```


## Scene 4

**Prompt:**  What might have caused drops in these classes? Assume that one of the causes might have been the amount of work assigned in the first two weeks of class. Create a simplified ideal Preceptor Table (using a spreadsheet of your choice) with no missing data which would allow us to investigate this situation. What data is missing and why? Create an actual Preceptor Table, again using a spreadsheet. How might we investigate the effect of work assigned in the first two weeks? Would the estimated Average Treatment Effect be accurate? Why or why not? Put some bullet points here and be prepared to show your spreadsheet to the class.


## Scene 5

Read in the data for all the available dates and use it to make a graphic which shows the changes in enrollment over time. The **gghighlight** package might be useful, perhaps to highlight what has happened in Gov 50, as compared to the other 500 or so courses.


